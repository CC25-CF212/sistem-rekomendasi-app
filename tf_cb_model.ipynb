{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc93e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Concatenate, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pickle\n",
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9956a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41b7fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostgreSQLContentBasedRecommender:\n",
    "    def __init__(self, db_config, model_path='models/'):\n",
    "        \"\"\"\n",
    "        Initialize the recommender with database configuration\n",
    "        \n",
    "        Args:\n",
    "            db_config (dict): PostgreSQL connection parameters\n",
    "            model_path (str): Path to save/load models\n",
    "        \"\"\"\n",
    "        self.db_config = db_config\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.similarity_matrix = None\n",
    "        self.articles_data = None\n",
    "        self.data_hash = None\n",
    "        \n",
    "        # Create model directory if it doesn't exist\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "    def get_db_connection(self):\n",
    "        \"\"\"Create and return database connection\"\"\"\n",
    "        try:\n",
    "            conn = psycopg2.connect(**self.db_config)\n",
    "            return conn\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database connection failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_data_from_db(self):\n",
    "        \"\"\"Load data from PostgreSQL database\"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        \n",
    "        try:\n",
    "            # Load articles data\n",
    "            articles_query = \"\"\"\n",
    "            SELECT id, title, slug, province, city, active, user_id, created_at, updated_at\n",
    "            FROM \"Articles\" as articles \n",
    "            WHERE active = true\n",
    "            ORDER BY created_at DESC\n",
    "            \"\"\"\n",
    "            articles_df = pd.read_sql(articles_query, conn)\n",
    "            \n",
    "            # Load likes data\n",
    "            likes_query = \"\"\"\n",
    "            SELECT id, article_id, user_id, created_at\n",
    "            FROM \"Article_likes\" as article_likes\n",
    "            \"\"\"\n",
    "            likes_df = pd.read_sql(likes_query, conn)\n",
    "            \n",
    "            # Load comments data\n",
    "            comments_query = \"\"\"\n",
    "            SELECT id, article_id, user_id, created_at\n",
    "            from \"Article_comments\" aS  article_comments\n",
    "            \"\"\"\n",
    "            comments_df = pd.read_sql(comments_query, conn)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(articles_df)} articles, {len(likes_df)} likes, {len(comments_df)} comments\")\n",
    "            \n",
    "            return articles_df, likes_df, comments_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data from database: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def calculate_data_hash(self, articles_df, likes_df, comments_df):\n",
    "        \"\"\"Calculate hash of the data to detect changes\"\"\"\n",
    "        data_string = (\n",
    "            str(articles_df.shape) + \n",
    "            str(likes_df.shape) + \n",
    "            str(comments_df.shape) +\n",
    "            str(articles_df['updated_at'].max()) +\n",
    "            str(likes_df['created_at'].max() if not likes_df.empty else '') +\n",
    "            str(comments_df['created_at'].max() if not comments_df.empty else '')\n",
    "        )\n",
    "        return hashlib.md5(data_string.encode()).hexdigest()\n",
    "    \n",
    "    def preprocess_data_old(self, articles_df, likes_df, comments_df):\n",
    "        \"\"\"Preprocess the loaded data\"\"\"\n",
    "        # Ensure all IDs are strings\n",
    "        articles_df['id'] = articles_df['id'].astype(str)\n",
    "        likes_df['article_id'] = likes_df['article_id'].astype(str)\n",
    "        comments_df['article_id'] = comments_df['article_id'].astype(str)\n",
    "        \n",
    "        # Calculate likes per article\n",
    "        article_likes = likes_df.groupby('article_id').size().reset_index(name='likes_count')\n",
    "        \n",
    "        # Calculate comments per article\n",
    "        article_comments = comments_df.groupby('article_id').size().reset_index(name='comments_count')\n",
    "        \n",
    "        # Combine data with articles\n",
    "        articles_enriched = articles_df.copy()\n",
    "        \n",
    "        # Add likes count\n",
    "        articles_enriched = articles_enriched.merge(\n",
    "            article_likes, left_on='id', right_on='article_id', how='left'\n",
    "        )\n",
    "        articles_enriched['likes_count'] = articles_enriched['likes_count'].fillna(0)\n",
    "        \n",
    "        # Add comments count\n",
    "        articles_enriched = articles_enriched.merge(\n",
    "            article_comments, left_on='id', right_on='article_id', how='left'\n",
    "        )\n",
    "        articles_enriched['comments_count'] = articles_enriched['comments_count'].fillna(0)\n",
    "        \n",
    "        # Clean up merge columns\n",
    "        articles_enriched = articles_enriched.drop(['article_id_x', 'article_id_y'], axis=1, errors='ignore')\n",
    "        \n",
    "        # Create text features\n",
    "        articles_enriched['text_features'] = (\n",
    "            articles_enriched['title'].fillna('') + ' ' + \n",
    "            articles_enriched['province'].fillna('') + ' ' + \n",
    "            articles_enriched['city'].fillna('')\n",
    "        )\n",
    "        \n",
    "        # Calculate engagement score\n",
    "        articles_enriched['engagement_score'] = (\n",
    "            articles_enriched['likes_count'] + (2 * articles_enriched['comments_count'])\n",
    "        )\n",
    "        \n",
    "        # Add time-based features\n",
    "        articles_enriched['created_at'] = pd.to_datetime(articles_enriched['created_at'])\n",
    "        articles_enriched['days_since_creation'] = (\n",
    "            (datetime.now() - articles_enriched['created_at']).dt.days\n",
    "        )\n",
    "        \n",
    "        # Calculate recency score (newer articles get higher scores)\n",
    "        max_days = articles_enriched['days_since_creation'].max()\n",
    "        articles_enriched['recency_score'] = 1 - (articles_enriched['days_since_creation'] / max_days)\n",
    "        \n",
    "        return articles_enriched\n",
    "    def preprocess_data(self, articles_df, likes_df, comments_df):\n",
    "        \"\"\"Preprocess the loaded data\"\"\"\n",
    "        # Ensure all IDs are strings\n",
    "        articles_df['id'] = articles_df['id'].astype(str)\n",
    "        likes_df['article_id'] = likes_df['article_id'].astype(str)\n",
    "        comments_df['article_id'] = comments_df['article_id'].astype(str)\n",
    "        \n",
    "        # Calculate likes per article\n",
    "        article_likes = likes_df.groupby('article_id').size().reset_index(name='likes_count')\n",
    "        \n",
    "        # Calculate comments per article\n",
    "        article_comments = comments_df.groupby('article_id').size().reset_index(name='comments_count')\n",
    "        \n",
    "        # Combine data with articles\n",
    "        articles_enriched = articles_df.copy()\n",
    "        \n",
    "        # Add likes count\n",
    "        articles_enriched = articles_enriched.merge(\n",
    "            article_likes, left_on='id', right_on='article_id', how='left'\n",
    "        )\n",
    "        articles_enriched['likes_count'] = articles_enriched['likes_count'].fillna(0)\n",
    "        \n",
    "        # Add comments count\n",
    "        articles_enriched = articles_enriched.merge(\n",
    "            article_comments, left_on='id', right_on='article_id', how='left'\n",
    "        )\n",
    "        articles_enriched['comments_count'] = articles_enriched['comments_count'].fillna(0)\n",
    "        \n",
    "        # Clean up merge columns\n",
    "        articles_enriched = articles_enriched.drop(['article_id_x', 'article_id_y'], axis=1, errors='ignore')\n",
    "        \n",
    "        # Create text features\n",
    "        articles_enriched['text_features'] = (\n",
    "            articles_enriched['title'].fillna('') + ' ' + \n",
    "            articles_enriched['province'].fillna('') + ' ' + \n",
    "            articles_enriched['city'].fillna('')\n",
    "        )\n",
    "        \n",
    "        # Calculate engagement score\n",
    "        articles_enriched['engagement_score'] = (\n",
    "            articles_enriched['likes_count'] + (2 * articles_enriched['comments_count'])\n",
    "        )\n",
    "        \n",
    "        print(articles_enriched.head(5))\n",
    "        print(articles_enriched.info())\n",
    "        # Add time-based features - FIX FOR TIMEZONE ISSUE\n",
    "        articles_enriched['created_at'] = pd.to_datetime(articles_enriched['created_at'])\n",
    "        \n",
    "        # Handle timezone-aware datetime comparison\n",
    "        if articles_enriched['created_at'].dt.tz is not None:\n",
    "            # Data is timezone-aware (UTC), so use UTC for current time\n",
    "            from datetime import timezone\n",
    "            now = datetime.now(timezone.utc)\n",
    "        else:\n",
    "            # Data is timezone-naive, use naive datetime\n",
    "            now = datetime.now()\n",
    "        \n",
    "        articles_enriched['days_since_creation'] = (\n",
    "            (now - articles_enriched['created_at']).dt.days\n",
    "        )\n",
    "        \n",
    "        # Calculate recency score (newer articles get higher scores)\n",
    "        max_days = articles_enriched['days_since_creation'].max()\n",
    "        if max_days > 0:\n",
    "            articles_enriched['recency_score'] = 1 - (articles_enriched['days_since_creation'] / max_days)\n",
    "        else:\n",
    "            # If all articles are from today, give them all the same recency score\n",
    "            articles_enriched['recency_score'] = 1.0\n",
    "        \n",
    "        return articles_enriched\n",
    "    def prepare_features(self, articles_enriched):\n",
    "        \"\"\"Prepare features for the model\"\"\"\n",
    "        # Text features using TF-IDF\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                max_features=1000,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2)\n",
    "            )\n",
    "            tfidf_features = self.tfidf_vectorizer.fit_transform(articles_enriched['text_features'])\n",
    "        else:\n",
    "            tfidf_features = self.tfidf_vectorizer.transform(articles_enriched['text_features'])\n",
    "        \n",
    "        # Categorical features\n",
    "        categorical_features = []\n",
    "        for col in ['province', 'city']:\n",
    "            if col not in self.label_encoders:\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "                encoded = self.label_encoders[col].fit_transform(articles_enriched[col].fillna('unknown'))\n",
    "            else:\n",
    "                # Handle unseen categories\n",
    "                known_classes = set(self.label_encoders[col].classes_)\n",
    "                articles_enriched[col] = articles_enriched[col].fillna('unknown')\n",
    "                articles_enriched.loc[~articles_enriched[col].isin(known_classes), col] = 'unknown'\n",
    "                encoded = self.label_encoders[col].transform(articles_enriched[col])\n",
    "            \n",
    "            categorical_features.append(encoded.reshape(-1, 1))\n",
    "        \n",
    "        # Numerical features\n",
    "        numerical_cols = ['likes_count', 'comments_count', 'engagement_score', 'recency_score']\n",
    "        numerical_features = articles_enriched[numerical_cols].values\n",
    "        \n",
    "        if not hasattr(self, 'scaler_fitted'):\n",
    "            numerical_features = self.scaler.fit_transform(numerical_features)\n",
    "            self.scaler_fitted = True\n",
    "        else:\n",
    "            numerical_features = self.scaler.transform(numerical_features)\n",
    "        \n",
    "        return tfidf_features, categorical_features, numerical_features\n",
    "    \n",
    "    def build_model(self, tfidf_dim, categorical_dims, numerical_dim):\n",
    "        \"\"\"Build the TensorFlow model\"\"\"\n",
    "        # Text input\n",
    "        text_input = Input(shape=(tfidf_dim,), name='text_input')\n",
    "        text_dense = Dense(128, activation='relu')(text_input)\n",
    "        text_dense = Dense(64, activation='relu')(text_dense)\n",
    "        \n",
    "        # Categorical inputs\n",
    "        categorical_inputs = []\n",
    "        categorical_embeddings = []\n",
    "        \n",
    "        for i, dim in enumerate(categorical_dims):\n",
    "            cat_input = Input(shape=(1,), name=f'cat_input_{i}')\n",
    "            cat_embedding = Embedding(dim, min(50, dim//2 + 1))(cat_input)\n",
    "            cat_embedding = Flatten()(cat_embedding)\n",
    "            categorical_inputs.append(cat_input)\n",
    "            categorical_embeddings.append(cat_embedding)\n",
    "        \n",
    "        # Numerical input\n",
    "        num_input = Input(shape=(numerical_dim,), name='num_input')\n",
    "        num_dense = Dense(32, activation='relu')(num_input)\n",
    "        \n",
    "        # Combine all features\n",
    "        if categorical_embeddings:\n",
    "            combined = Concatenate()([text_dense] + categorical_embeddings + [num_dense])\n",
    "        else:\n",
    "            combined = Concatenate()([text_dense, num_dense])\n",
    "        \n",
    "        # Final layers\n",
    "        combined = Dense(128, activation='relu')(combined)\n",
    "        combined = Dense(64, activation='relu')(combined)\n",
    "        output = Dense(32, activation='linear', name='content_embedding')(combined)\n",
    "        \n",
    "        # Create model\n",
    "        all_inputs = [text_input] + categorical_inputs + [num_input]\n",
    "        model = tf.keras.Model(inputs=all_inputs, outputs=output)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def should_retrain_old(self, current_hash):\n",
    "        \"\"\"Check if model should be retrained\"\"\"\n",
    "        if self.data_hash is None or self.data_hash != current_hash:\n",
    "            return True\n",
    "        \n",
    "        model_files = [\n",
    "            os.path.join(self.model_path, 'content_model.h5'),\n",
    "            os.path.join(self.model_path, 'tfidf_vectorizer.pkl'),\n",
    "            os.path.join(self.model_path, 'scaler.pkl'),\n",
    "            os.path.join(self.model_path, 'label_encoders.pkl')\n",
    "        ]\n",
    "        \n",
    "        return not all(os.path.exists(f) for f in model_files)\n",
    "    def should_retrain(self, current_hash):\n",
    "        \"\"\"Check if model should be retrained\"\"\"\n",
    "        if self.data_hash is None or self.data_hash != current_hash:\n",
    "            return True\n",
    "        \n",
    "        # Check for both new and old model formats\n",
    "        model_files = [\n",
    "            os.path.join(self.model_path, 'tfidf_vectorizer.pkl'),\n",
    "            os.path.join(self.model_path, 'scaler.pkl'),\n",
    "            os.path.join(self.model_path, 'label_encoders.pkl')\n",
    "        ]\n",
    "        \n",
    "        # Check if either model format exists\n",
    "        keras_model_exists = os.path.exists(os.path.join(self.model_path, 'content_model.keras'))\n",
    "        h5_model_exists = os.path.exists(os.path.join(self.model_path, 'content_model.h5'))\n",
    "        \n",
    "        model_exists = keras_model_exists or h5_model_exists\n",
    "        other_files_exist = all(os.path.exists(f) for f in model_files)\n",
    "        \n",
    "        return not (model_exists and other_files_exist)\n",
    "    \n",
    "    def fit(self, force_retrain=False):\n",
    "        \"\"\"Fit the recommender model\"\"\"\n",
    "        # Load data from database\n",
    "        articles_df, likes_df, comments_df = self.load_data_from_db()\n",
    "        \n",
    "        # Calculate data hash\n",
    "        current_hash = self.calculate_data_hash(articles_df, likes_df, comments_df)\n",
    "        \n",
    "        # Check if retraining is needed\n",
    "        if not force_retrain and not self.should_retrain(current_hash):\n",
    "            logger.info(\"No changes detected in data. Loading existing model...\")\n",
    "            self.load_model()\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Training model with new/updated data...\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        self.articles_data = self.preprocess_data(articles_df, likes_df, comments_df)\n",
    "        \n",
    "        # Prepare features\n",
    "        tfidf_features, categorical_features, numerical_features = self.prepare_features(self.articles_data)\n",
    "        \n",
    "        # Build model\n",
    "        tfidf_dim = tfidf_features.shape[1]\n",
    "        categorical_dims = [len(encoder.classes_) for encoder in self.label_encoders.values()]\n",
    "        numerical_dim = numerical_features.shape[1]\n",
    "        \n",
    "        self.model = self.build_model(tfidf_dim, categorical_dims, numerical_dim)\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_text = tfidf_features.toarray()\n",
    "        X_categorical = [cat_feat.flatten() for cat_feat in categorical_features]\n",
    "        X_numerical = numerical_features\n",
    "        \n",
    "        # Create target (we'll use the features themselves for autoencoder-like training)\n",
    "        y = np.concatenate([X_text[:, :32], X_numerical], axis=1)\n",
    "        if y.shape[1] > 32:\n",
    "            y = y[:, :32]\n",
    "        elif y.shape[1] < 32:\n",
    "            y = np.pad(y, ((0, 0), (0, 32 - y.shape[1])), mode='constant')\n",
    "        \n",
    "        # Train model\n",
    "        X_train = [X_text] + X_categorical + [X_numerical]\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_train, y,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        embeddings = self.model.predict(X_train)\n",
    "        self.similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Update data hash\n",
    "        self.data_hash = current_hash\n",
    "        \n",
    "        # Save model\n",
    "        self.save_model()\n",
    "        \n",
    "        logger.info(\"Model training completed and saved!\")\n",
    "    \n",
    "    def recommend(self, article_id, top_n=5):\n",
    "        \"\"\"Generate recommendations for a given article\"\"\"\n",
    "        if self.model is None or self.articles_data is None:\n",
    "            self.fit()\n",
    "        \n",
    "        # Find article index\n",
    "        article_idx = self.articles_data[self.articles_data['id'] == str(article_id)].index\n",
    "        \n",
    "        if len(article_idx) == 0:\n",
    "            logger.warning(f\"Article ID {article_id} not found\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        article_idx = article_idx[0]\n",
    "        \n",
    "        # Get similarity scores\n",
    "        sim_scores = list(enumerate(self.similarity_matrix[article_idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top recommendations (excluding the article itself)\n",
    "        sim_scores = sim_scores[1:top_n+1]\n",
    "        article_indices = [i[0] for i in sim_scores]\n",
    "        \n",
    "        # Return recommended articles\n",
    "        recommendations = self.articles_data.iloc[article_indices].copy()\n",
    "        recommendations['similarity_score'] = [score[1] for score in sim_scores]\n",
    "        \n",
    "        return recommendations[['id', 'title', 'province', 'city', 'engagement_score', 'similarity_score']]\n",
    "    \n",
    "    def save_model_old(self):\n",
    "        \"\"\"Save the trained model and preprocessors\"\"\"\n",
    "        # Save TensorFlow model\n",
    "        self.model.save(os.path.join(self.model_path, 'content_model.h5'))\n",
    "        \n",
    "        # Save preprocessors\n",
    "        with open(os.path.join(self.model_path, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.tfidf_vectorizer, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'scaler.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'label_encoders.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.label_encoders, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'similarity_matrix.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.similarity_matrix, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'articles_data.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.articles_data, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'data_hash.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.data_hash, f)\n",
    "        \n",
    "        logger.info(\"Model and preprocessors saved successfully!\")\n",
    "    def save_model(self):\n",
    "        \"\"\"Save the trained model and preprocessors\"\"\"\n",
    "        # Save TensorFlow model in native Keras format (recommended)\n",
    "        self.model.save(os.path.join(self.model_path, 'content_model.keras'))\n",
    "        \n",
    "        # Save preprocessors\n",
    "        with open(os.path.join(self.model_path, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.tfidf_vectorizer, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'scaler.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'label_encoders.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.label_encoders, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'similarity_matrix.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.similarity_matrix, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'articles_data.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.articles_data, f)\n",
    "        \n",
    "        with open(os.path.join(self.model_path, 'data_hash.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.data_hash, f)\n",
    "        \n",
    "        logger.info(\"Model and preprocessors saved successfully!\")    \n",
    "    \n",
    "    def load_model_old(self):\n",
    "        \"\"\"Load the trained model and preprocessors\"\"\"\n",
    "        try:\n",
    "            # Load TensorFlow model\n",
    "            self.model = load_model(os.path.join(self.model_path, 'content_model.h5'))\n",
    "            \n",
    "            # Load preprocessors\n",
    "            with open(os.path.join(self.model_path, 'tfidf_vectorizer.pkl'), 'rb') as f:\n",
    "                self.tfidf_vectorizer = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'scaler.pkl'), 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "                self.scaler_fitted = True\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'label_encoders.pkl'), 'rb') as f:\n",
    "                self.label_encoders = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'similarity_matrix.pkl'), 'rb') as f:\n",
    "                self.similarity_matrix = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'articles_data.pkl'), 'rb') as f:\n",
    "                self.articles_data = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'data_hash.pkl'), 'rb') as f:\n",
    "                self.data_hash = pickle.load(f)\n",
    "            \n",
    "            logger.info(\"Model and preprocessors loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model and preprocessors\"\"\"\n",
    "        try:\n",
    "            # Try to load new format first, then fallback to old format\n",
    "            keras_model_path = os.path.join(self.model_path, 'content_model.keras')\n",
    "            h5_model_path = os.path.join(self.model_path, 'content_model.h5')\n",
    "            \n",
    "            if os.path.exists(keras_model_path):\n",
    "                # Load TensorFlow model (new format)\n",
    "                self.model = load_model(keras_model_path)\n",
    "            elif os.path.exists(h5_model_path):\n",
    "                # Load TensorFlow model (old format)\n",
    "                self.model = load_model(h5_model_path)\n",
    "                logger.warning(\"Loaded model from legacy HDF5 format. Consider retraining to save in new format.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No model file found (neither .keras nor .h5)\")\n",
    "            \n",
    "            # Load preprocessors\n",
    "            with open(os.path.join(self.model_path, 'tfidf_vectorizer.pkl'), 'rb') as f:\n",
    "                self.tfidf_vectorizer = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'scaler.pkl'), 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "                self.scaler_fitted = True\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'label_encoders.pkl'), 'rb') as f:\n",
    "                self.label_encoders = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'similarity_matrix.pkl'), 'rb') as f:\n",
    "                self.similarity_matrix = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'articles_data.pkl'), 'rb') as f:\n",
    "                self.articles_data = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(self.model_path, 'data_hash.pkl'), 'rb') as f:\n",
    "                self.data_hash = pickle.load(f)\n",
    "            \n",
    "            logger.info(\"Model and preprocessors loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            raise       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90a7b448",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Database configuration\n",
    "db_config = {\n",
    "        'host': 'ep-tight-mountain-a1oxjc3h-pooler.ap-southeast-1.aws.neon.tech',\n",
    "        'database': 'db_artikel',\n",
    "        'user': 'neondb_owner',\n",
    "        'password': 'npg_4XAZOBI8qjWk',\n",
    "        'port': 5432\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b9bdac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "db_config = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'database': os.getenv('DB_NAME'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432))  # default ke 5432\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34c0f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recommender\n",
    "recommender = PostgreSQLContentBasedRecommender(db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9098a105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MID-020\\AppData\\Local\\Temp\\ipykernel_14340\\339857099.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  articles_df = pd.read_sql(articles_query, conn)\n",
      "C:\\Users\\MID-020\\AppData\\Local\\Temp\\ipykernel_14340\\339857099.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  likes_df = pd.read_sql(likes_query, conn)\n",
      "C:\\Users\\MID-020\\AppData\\Local\\Temp\\ipykernel_14340\\339857099.py:58: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  comments_df = pd.read_sql(comments_query, conn)\n",
      "INFO:__main__:Loaded 48 articles, 4 likes, 1 comments\n",
      "INFO:__main__:Training model with new/updated data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  \\\n",
      "0  8959dc75-d87a-4c13-9fc7-feead6b13b45   \n",
      "1  6b1de6e6-d2ac-4094-ae81-1d9c39551e61   \n",
      "2  d287687c-a50a-42d0-93db-2bcb2fb43ec8   \n",
      "3  026b1b59-e6d1-42d3-a4ff-42967b55a526   \n",
      "4  0fb0bc6c-2d97-4b18-a6cd-ef5fd3f1414f   \n",
      "\n",
      "                                             title  \\\n",
      "0  Cagar Alam Kawah Ijen di Banyuwangi, Jawa Timur   \n",
      "1      Cagar Alam Maninjau di Agam, Sumatera Barat   \n",
      "2                                       Seren Raun   \n",
      "3     Tradisi Pasola di Sumba, Nusa Tenggara Timur   \n",
      "4                Suku Asmat dan Seni Ukir di Papua   \n",
      "\n",
      "                                             slug             province  \\\n",
      "0  cagar-alam-kawah-ijen-di-banyuwangi-jawa-timur           Jawa Timur   \n",
      "1      cagar-alam-maninjau-di-agam-sumatera-barat       Sumatera Barat   \n",
      "2                                      seren-raun               Banten   \n",
      "3     tradisi-pasola-di-sumba-nusa-tenggara-timur  Nusa Tenggara Timur   \n",
      "4               suku-asmat-dan-seni-ukir-di-papua                Papua   \n",
      "\n",
      "                   city  active                               user_id  \\\n",
      "0  Kabupaten Banyuwangi    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "1        Kabupaten Agam    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "2       Kabupaten Lebak    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "3        Kabupaten Alor    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "4       Kabupaten Asmat    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "\n",
      "                        created_at                       updated_at  \\\n",
      "0 2025-06-05 16:47:27.297000+00:00 2025-06-05 16:47:27.297000+00:00   \n",
      "1 2025-06-05 16:45:09.781000+00:00 2025-06-05 16:45:09.781000+00:00   \n",
      "2 2025-06-05 16:41:57.666000+00:00 2025-06-05 16:41:57.666000+00:00   \n",
      "3 2025-06-05 16:37:24.465000+00:00 2025-06-05 16:37:24.465000+00:00   \n",
      "4 2025-06-05 16:34:38.517000+00:00 2025-06-05 16:34:38.517000+00:00   \n",
      "\n",
      "   likes_count  comments_count  \\\n",
      "0          0.0             1.0   \n",
      "1          0.0             0.0   \n",
      "2          0.0             0.0   \n",
      "3          0.0             0.0   \n",
      "4          0.0             0.0   \n",
      "\n",
      "                                       text_features  engagement_score  \n",
      "0  Cagar Alam Kawah Ijen di Banyuwangi, Jawa Timu...               2.0  \n",
      "1  Cagar Alam Maninjau di Agam, Sumatera Barat Su...               0.0  \n",
      "2                  Seren Raun Banten Kabupaten Lebak               0.0  \n",
      "3   Tradisi Pasola di Sumba, Nusa Tenggara Timur ...               0.0  \n",
      "4   Suku Asmat dan Seni Ukir di Papua Papua Kabup...               0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48 entries, 0 to 47\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   id                48 non-null     object             \n",
      " 1   title             48 non-null     object             \n",
      " 2   slug              48 non-null     object             \n",
      " 3   province          48 non-null     object             \n",
      " 4   city              48 non-null     object             \n",
      " 5   active            48 non-null     bool               \n",
      " 6   user_id           48 non-null     object             \n",
      " 7   created_at        48 non-null     datetime64[ns, UTC]\n",
      " 8   updated_at        48 non-null     datetime64[ns, UTC]\n",
      " 9   likes_count       48 non-null     float64            \n",
      " 10  comments_count    48 non-null     float64            \n",
      " 11  text_features     48 non-null     object             \n",
      " 12  engagement_score  48 non-null     float64            \n",
      "dtypes: bool(1), datetime64[ns, UTC](2), float64(3), object(7)\n",
      "memory usage: 4.7+ KB\n",
      "None\n",
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 489ms/step - loss: 0.0168 - mae: 0.0571 - val_loss: 0.0149 - val_mae: 0.0793\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.0102 - mae: 0.0405 - val_loss: 0.0105 - val_mae: 0.0637\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0052 - mae: 0.0297 - val_loss: 0.0080 - val_mae: 0.0537\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0051 - mae: 0.0272 - val_loss: 0.0064 - val_mae: 0.0470\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0042 - mae: 0.0250 - val_loss: 0.0054 - val_mae: 0.0426\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544ms/step - loss: 0.0037 - mae: 0.0239 - val_loss: 0.0047 - val_mae: 0.0401\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.0032 - mae: 0.0221 - val_loss: 0.0043 - val_mae: 0.0383\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0029 - mae: 0.0209 - val_loss: 0.0041 - val_mae: 0.0371\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.0028 - mae: 0.0204 - val_loss: 0.0039 - val_mae: 0.0361\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0026 - mae: 0.0196 - val_loss: 0.0038 - val_mae: 0.0353\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0023 - mae: 0.0187 - val_loss: 0.0037 - val_mae: 0.0348\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0023 - mae: 0.0188 - val_loss: 0.0036 - val_mae: 0.0342\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0023 - mae: 0.0189 - val_loss: 0.0036 - val_mae: 0.0337\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0020 - mae: 0.0180 - val_loss: 0.0036 - val_mae: 0.0332\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0020 - mae: 0.0179 - val_loss: 0.0036 - val_mae: 0.0330\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0019 - mae: 0.0180 - val_loss: 0.0036 - val_mae: 0.0330\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0019 - mae: 0.0180 - val_loss: 0.0037 - val_mae: 0.0333\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0019 - mae: 0.0181 - val_loss: 0.0038 - val_mae: 0.0337\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0018 - mae: 0.0180 - val_loss: 0.0038 - val_mae: 0.0341\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0017 - mae: 0.0176 - val_loss: 0.0039 - val_mae: 0.0343\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0018 - mae: 0.0185 - val_loss: 0.0039 - val_mae: 0.0344\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.0017 - mae: 0.0182 - val_loss: 0.0039 - val_mae: 0.0342\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0017 - mae: 0.0179 - val_loss: 0.0039 - val_mae: 0.0339\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.0016 - mae: 0.0178 - val_loss: 0.0039 - val_mae: 0.0340\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.0016 - mae: 0.0179 - val_loss: 0.0040 - val_mae: 0.0346\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0016 - mae: 0.0178 - val_loss: 0.0041 - val_mae: 0.0355\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0015 - mae: 0.0177 - val_loss: 0.0041 - val_mae: 0.0361\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0015 - mae: 0.0179 - val_loss: 0.0042 - val_mae: 0.0363\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0014 - mae: 0.0173 - val_loss: 0.0042 - val_mae: 0.0367\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.0014 - mae: 0.0178 - val_loss: 0.0043 - val_mae: 0.0373\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.0013 - mae: 0.0174 - val_loss: 0.0043 - val_mae: 0.0377\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.0013 - mae: 0.0171 - val_loss: 0.0043 - val_mae: 0.0376\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.0012 - mae: 0.0170 - val_loss: 0.0043 - val_mae: 0.0377\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0012 - mae: 0.0173 - val_loss: 0.0043 - val_mae: 0.0379\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0011 - mae: 0.0167 - val_loss: 0.0043 - val_mae: 0.0385\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0011 - mae: 0.0163 - val_loss: 0.0043 - val_mae: 0.0391\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.0011 - mae: 0.0164 - val_loss: 0.0044 - val_mae: 0.0397\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0010 - mae: 0.0162 - val_loss: 0.0045 - val_mae: 0.0400\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 9.8725e-04 - mae: 0.0157 - val_loss: 0.0046 - val_mae: 0.0405\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 9.9422e-04 - mae: 0.0161 - val_loss: 0.0048 - val_mae: 0.0409\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 9.4812e-04 - mae: 0.0160 - val_loss: 0.0049 - val_mae: 0.0411\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 8.6873e-04 - mae: 0.0154 - val_loss: 0.0050 - val_mae: 0.0414\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 8.4020e-04 - mae: 0.0152 - val_loss: 0.0050 - val_mae: 0.0419\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 8.3573e-04 - mae: 0.0154 - val_loss: 0.0051 - val_mae: 0.0428\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 7.4673e-04 - mae: 0.0149 - val_loss: 0.0052 - val_mae: 0.0434\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 8.1698e-04 - mae: 0.0160 - val_loss: 0.0052 - val_mae: 0.0435\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 7.7246e-04 - mae: 0.0156 - val_loss: 0.0051 - val_mae: 0.0431\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 6.9878e-04 - mae: 0.0147 - val_loss: 0.0052 - val_mae: 0.0429\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 7.0473e-04 - mae: 0.0148 - val_loss: 0.0052 - val_mae: 0.0431\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 6.1979e-04 - mae: 0.0140 - val_loss: 0.0052 - val_mae: 0.0431\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model and preprocessors saved successfully!\n",
      "INFO:__main__:Model training completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Fit the model (will automatically check if retraining is needed)\n",
    "recommender.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37e11b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations\n",
    "article_id = '8959dc75-d87a-4c13-9fc7-feead6b13b45'\n",
    "recommendations = recommender.recommend(article_id, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a504bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rekomendasi untuk artikel ID '8959dc75-d87a-4c13-9fc7-feead6b13b45':\n",
      "- 73294f35-0382-4af7-8930-476919f550d9 (Skor: 0.3888)\n",
      "- Cagar Alam Lembah Harau: Keindahan Alam yang Menakjubkan (Skor: 0.3888)\n",
      "- 026b1b59-e6d1-42d3-a4ff-42967b55a526 (Skor: 0.2204)\n",
      "-  Tradisi Pasola di Sumba, Nusa Tenggara Timur (Skor: 0.2204)\n",
      "- 27bbfb6f-5fd1-4a54-8317-f95f481ecfd7 (Skor: 0.1833)\n",
      "- Taman Nasional Ujung Kulon (Skor: 0.1833)\n",
      "- aa6a9d8c-797d-4c3b-a1af-59e2594f421e (Skor: 0.1672)\n",
      "- Upacara Ngaben di Bali (Skor: 0.1672)\n",
      "- 6b1de6e6-d2ac-4094-ae81-1d9c39551e61 (Skor: 0.1547)\n",
      "- Cagar Alam Maninjau di Agam, Sumatera Barat (Skor: 0.1547)\n",
      "- d764ffa1-9d2d-4d2e-98f5-142c120df86e (Skor: 0.1505)\n",
      "- Bunaken (Skor: 0.1505)\n",
      "- 58feb5ad-f9ba-487f-bb7f-8bde07fa8733 (Skor: 0.1417)\n",
      "- Jelajah Keraton Surakarta (Skor: 0.1417)\n",
      "- 4399b0cb-a105-4207-a257-970a199d1d6a (Skor: 0.1262)\n",
      "- Tugu JOGJA (Skor: 0.1262)\n",
      "- 8474948f-5892-4035-9e26-9841e1286cda (Skor: 0.1047)\n",
      "- Keindahan Pantai Carocok: Surga Bahari di Pesisir Selatan (Skor: 0.1047)\n",
      "- 86c96f4f-956e-4a65-8ba6-ba75b772619f (Skor: 0.0999)\n",
      "- Tari Barong di Desa Batubulan (Skor: 0.0999)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rekomendasi untuk artikel ID '{article_id}':\")\n",
    "for _, row in recommendations.iterrows():\n",
    "    print(f\"- {row['id']} (Skor: {row['similarity_score']:.4f})\")\n",
    "    print(f\"- {row['title']} (Skor: {row['similarity_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f981a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MID-020\\AppData\\Local\\Temp\\ipykernel_14340\\339857099.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  articles_df = pd.read_sql(articles_query, conn)\n",
      "C:\\Users\\MID-020\\AppData\\Local\\Temp\\ipykernel_14340\\339857099.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  likes_df = pd.read_sql(likes_query, conn)\n",
      "C:\\Users\\MID-020\\AppData\\Local\\Temp\\ipykernel_14340\\339857099.py:58: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  comments_df = pd.read_sql(comments_query, conn)\n",
      "INFO:__main__:Loaded 48 articles, 4 likes, 1 comments\n",
      "INFO:__main__:Training model with new/updated data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  \\\n",
      "0  8959dc75-d87a-4c13-9fc7-feead6b13b45   \n",
      "1  6b1de6e6-d2ac-4094-ae81-1d9c39551e61   \n",
      "2  d287687c-a50a-42d0-93db-2bcb2fb43ec8   \n",
      "3  026b1b59-e6d1-42d3-a4ff-42967b55a526   \n",
      "4  0fb0bc6c-2d97-4b18-a6cd-ef5fd3f1414f   \n",
      "\n",
      "                                             title  \\\n",
      "0  Cagar Alam Kawah Ijen di Banyuwangi, Jawa Timur   \n",
      "1      Cagar Alam Maninjau di Agam, Sumatera Barat   \n",
      "2                                       Seren Raun   \n",
      "3     Tradisi Pasola di Sumba, Nusa Tenggara Timur   \n",
      "4                Suku Asmat dan Seni Ukir di Papua   \n",
      "\n",
      "                                             slug             province  \\\n",
      "0  cagar-alam-kawah-ijen-di-banyuwangi-jawa-timur           Jawa Timur   \n",
      "1      cagar-alam-maninjau-di-agam-sumatera-barat       Sumatera Barat   \n",
      "2                                      seren-raun               Banten   \n",
      "3     tradisi-pasola-di-sumba-nusa-tenggara-timur  Nusa Tenggara Timur   \n",
      "4               suku-asmat-dan-seni-ukir-di-papua                Papua   \n",
      "\n",
      "                   city  active                               user_id  \\\n",
      "0  Kabupaten Banyuwangi    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "1        Kabupaten Agam    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "2       Kabupaten Lebak    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "3        Kabupaten Alor    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "4       Kabupaten Asmat    True  bd2fbcfe-11fc-4018-b1ca-9c447c470e58   \n",
      "\n",
      "                        created_at                       updated_at  \\\n",
      "0 2025-06-05 16:47:27.297000+00:00 2025-06-05 16:47:27.297000+00:00   \n",
      "1 2025-06-05 16:45:09.781000+00:00 2025-06-05 16:45:09.781000+00:00   \n",
      "2 2025-06-05 16:41:57.666000+00:00 2025-06-05 16:41:57.666000+00:00   \n",
      "3 2025-06-05 16:37:24.465000+00:00 2025-06-05 16:37:24.465000+00:00   \n",
      "4 2025-06-05 16:34:38.517000+00:00 2025-06-05 16:34:38.517000+00:00   \n",
      "\n",
      "   likes_count  comments_count  \\\n",
      "0          0.0             1.0   \n",
      "1          0.0             0.0   \n",
      "2          0.0             0.0   \n",
      "3          0.0             0.0   \n",
      "4          0.0             0.0   \n",
      "\n",
      "                                       text_features  engagement_score  \n",
      "0  Cagar Alam Kawah Ijen di Banyuwangi, Jawa Timu...               2.0  \n",
      "1  Cagar Alam Maninjau di Agam, Sumatera Barat Su...               0.0  \n",
      "2                  Seren Raun Banten Kabupaten Lebak               0.0  \n",
      "3   Tradisi Pasola di Sumba, Nusa Tenggara Timur ...               0.0  \n",
      "4   Suku Asmat dan Seni Ukir di Papua Papua Kabup...               0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48 entries, 0 to 47\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   id                48 non-null     object             \n",
      " 1   title             48 non-null     object             \n",
      " 2   slug              48 non-null     object             \n",
      " 3   province          48 non-null     object             \n",
      " 4   city              48 non-null     object             \n",
      " 5   active            48 non-null     bool               \n",
      " 6   user_id           48 non-null     object             \n",
      " 7   created_at        48 non-null     datetime64[ns, UTC]\n",
      " 8   updated_at        48 non-null     datetime64[ns, UTC]\n",
      " 9   likes_count       48 non-null     float64            \n",
      " 10  comments_count    48 non-null     float64            \n",
      " 11  text_features     48 non-null     object             \n",
      " 12  engagement_score  48 non-null     float64            \n",
      "dtypes: bool(1), datetime64[ns, UTC](2), float64(3), object(7)\n",
      "memory usage: 4.7+ KB\n",
      "None\n",
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 341ms/step - loss: 0.0129 - mae: 0.0573 - val_loss: 0.0227 - val_mae: 0.1022\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 0.0078 - mae: 0.0425 - val_loss: 0.0155 - val_mae: 0.0818\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0054 - mae: 0.0343 - val_loss: 0.0116 - val_mae: 0.0682\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.0043 - mae: 0.0294 - val_loss: 0.0095 - val_mae: 0.0604\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0036 - mae: 0.0268 - val_loss: 0.0082 - val_mae: 0.0559\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0027 - mae: 0.0229 - val_loss: 0.0074 - val_mae: 0.0531\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0028 - mae: 0.0228 - val_loss: 0.0069 - val_mae: 0.0512\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0023 - mae: 0.0202 - val_loss: 0.0065 - val_mae: 0.0497\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0023 - mae: 0.0195 - val_loss: 0.0062 - val_mae: 0.0479\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0022 - mae: 0.0188 - val_loss: 0.0058 - val_mae: 0.0455\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0021 - mae: 0.0189 - val_loss: 0.0055 - val_mae: 0.0431\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - loss: 0.0020 - mae: 0.0183 - val_loss: 0.0052 - val_mae: 0.0416\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0020 - mae: 0.0185 - val_loss: 0.0051 - val_mae: 0.0404\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0018 - mae: 0.0177 - val_loss: 0.0049 - val_mae: 0.0394\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 0.0019 - mae: 0.0180 - val_loss: 0.0048 - val_mae: 0.0386\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0017 - mae: 0.0174 - val_loss: 0.0048 - val_mae: 0.0380\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.0017 - mae: 0.0175 - val_loss: 0.0049 - val_mae: 0.0378\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0016 - mae: 0.0171 - val_loss: 0.0050 - val_mae: 0.0379\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0015 - mae: 0.0169 - val_loss: 0.0051 - val_mae: 0.0384\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0015 - mae: 0.0173 - val_loss: 0.0052 - val_mae: 0.0390\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0015 - mae: 0.0179 - val_loss: 0.0053 - val_mae: 0.0395\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - loss: 0.0014 - mae: 0.0173 - val_loss: 0.0054 - val_mae: 0.0397\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0014 - mae: 0.0178 - val_loss: 0.0054 - val_mae: 0.0398\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0012 - mae: 0.0167 - val_loss: 0.0054 - val_mae: 0.0397\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0012 - mae: 0.0171 - val_loss: 0.0054 - val_mae: 0.0392\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0012 - mae: 0.0169 - val_loss: 0.0054 - val_mae: 0.0389\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0011 - mae: 0.0172 - val_loss: 0.0054 - val_mae: 0.0389\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - loss: 0.0011 - mae: 0.0177 - val_loss: 0.0055 - val_mae: 0.0391\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - loss: 0.0011 - mae: 0.0179 - val_loss: 0.0055 - val_mae: 0.0391\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0010 - mae: 0.0176 - val_loss: 0.0055 - val_mae: 0.0388\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 9.9580e-04 - mae: 0.0173 - val_loss: 0.0055 - val_mae: 0.0387\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 9.3599e-04 - mae: 0.0168 - val_loss: 0.0056 - val_mae: 0.0386\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 9.3811e-04 - mae: 0.0166 - val_loss: 0.0056 - val_mae: 0.0386\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 8.9103e-04 - mae: 0.0162 - val_loss: 0.0056 - val_mae: 0.0386\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 8.0635e-04 - mae: 0.0154 - val_loss: 0.0057 - val_mae: 0.0387\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 8.3476e-04 - mae: 0.0158 - val_loss: 0.0057 - val_mae: 0.0387\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 7.2102e-04 - mae: 0.0149 - val_loss: 0.0057 - val_mae: 0.0385\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 7.4075e-04 - mae: 0.0151 - val_loss: 0.0057 - val_mae: 0.0381\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 6.8196e-04 - mae: 0.0147 - val_loss: 0.0057 - val_mae: 0.0379\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 6.1533e-04 - mae: 0.0141 - val_loss: 0.0058 - val_mae: 0.0379\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 6.2621e-04 - mae: 0.0144 - val_loss: 0.0058 - val_mae: 0.0380\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 6.2693e-04 - mae: 0.0146 - val_loss: 0.0059 - val_mae: 0.0380\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - loss: 5.7191e-04 - mae: 0.0140 - val_loss: 0.0059 - val_mae: 0.0379\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5.3756e-04 - mae: 0.0136 - val_loss: 0.0059 - val_mae: 0.0376\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 5.0633e-04 - mae: 0.0132 - val_loss: 0.0059 - val_mae: 0.0374\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 4.7216e-04 - mae: 0.0129 - val_loss: 0.0060 - val_mae: 0.0375\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 4.6513e-04 - mae: 0.0129 - val_loss: 0.0060 - val_mae: 0.0380\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 4.3191e-04 - mae: 0.0125 - val_loss: 0.0061 - val_mae: 0.0383\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 4.2958e-04 - mae: 0.0124 - val_loss: 0.0061 - val_mae: 0.0383\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 3.9094e-04 - mae: 0.0120 - val_loss: 0.0061 - val_mae: 0.0381\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model and preprocessors saved successfully!\n",
      "INFO:__main__:Model training completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Force retrain if needed\n",
    "recommender.fit(force_retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3355846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations\n",
    "article_id = '8959dc75-d87a-4c13-9fc7-feead6b13b45'\n",
    "recommendations = recommender.recommend(article_id, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1958ccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rekomendasi untuk artikel ID '8959dc75-d87a-4c13-9fc7-feead6b13b45':\n",
      "- Cagar Alam Lembah Harau: Keindahan Alam yang Menakjubkan (Skor: 0.3888)\n",
      "-  Tradisi Pasola di Sumba, Nusa Tenggara Timur (Skor: 0.2204)\n",
      "- Taman Nasional Ujung Kulon (Skor: 0.1833)\n",
      "- Upacara Ngaben di Bali (Skor: 0.1672)\n",
      "- Cagar Alam Maninjau di Agam, Sumatera Barat (Skor: 0.1547)\n",
      "- Bunaken (Skor: 0.1505)\n",
      "- Jelajah Keraton Surakarta (Skor: 0.1417)\n",
      "- Tugu JOGJA (Skor: 0.1262)\n",
      "- Keindahan Pantai Carocok: Surga Bahari di Pesisir Selatan (Skor: 0.1047)\n",
      "- Tari Barong di Desa Batubulan (Skor: 0.0999)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rekomendasi untuk artikel ID '{article_id}':\")\n",
    "for _, row in recommendations.iterrows():\n",
    "    print(f\"- {row['title']} (Skor: {row['similarity_score']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
